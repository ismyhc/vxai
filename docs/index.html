<!doctype html>
<html lang="en">

<head>
	<meta charset="UTF-8" />
	<meta http-equiv="x-ua-compatible" content="IE=edge" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
	<title>vxai | vdoc</title>
	<link rel="preconnect" href="https://fonts.googleapis.com" />
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
	<link href="https://fonts.googleapis.com/css2?family=Jost:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
	<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono&display=swap" rel="stylesheet" />
	<link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png" />
	<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png" />
	<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png" />
	<link rel="manifest" href="site.webmanifest" />
	<link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5" />
	<meta name="msapplication-TileColor" content="#da532c" />
	<meta name="theme-color" content="#ffffff" />
	<link rel="stylesheet" href="doc.css" />
	<link rel="stylesheet" href="normalize.css" />
	<script src="dark-mode.js"></script>
</head>

<body>
	<div><a id="skip-to-content-link" href="#main-content">Skip to content</a></div>
	<div id="page">
		<header class="doc-nav hidden">
			<div class="heading-container">
				<div class="heading">
					<div class="info">
						<div class="module">vxai</div>
						<div class="toggle-version-container">
							<span>0.1.0 6160945</span>
							<div id="dark-mode-toggle" role="switch" aria-checked="false" aria-label="Toggle dark mode">
								<svg xmlns="http://www.w3.org/2000/svg" class="light-icon" height="24"
									viewBox="0 0 24 24" width="24">
									<path d="M0 0h24v24H0z" fill="none" />
									<path
										d="M6.76 4.84l-1.8-1.79-1.41 1.41 1.79 1.79 1.42-1.41zM4 10.5H1v2h3v-2zm9-9.95h-2V3.5h2V.55zm7.45 3.91l-1.41-1.41-1.79 1.79 1.41 1.41 1.79-1.79zm-3.21 13.7l1.79 1.8 1.41-1.41-1.8-1.79-1.4 1.4zM20 10.5v2h3v-2h-3zm-8-5c-3.31 0-6 2.69-6 6s2.69 6 6 6 6-2.69 6-6-2.69-6-6-6zm-1 16.95h2V19.5h-2v2.95zm-7.45-3.91l1.41 1.41 1.79-1.8-1.41-1.41-1.79 1.8z" />
								</svg>
								<svg xmlns="http://www.w3.org/2000/svg" class="dark-icon"
									enable-background="new 0 0 24 24" height="24" viewBox="0 0 24 24" width="24">
									<g>
										<rect fill="none" height="24" width="24" />
									</g>
									<g>
										<g>
											<g>
												<path
													d="M11.1,12.08C8.77,7.57,10.6,3.6,11.63,2.01C6.27,2.2,1.98,6.59,1.98,12c0,0.14,0.02,0.28,0.02,0.42 C2.62,12.15,3.29,12,4,12c1.66,0,3.18,0.83,4.1,2.15C9.77,14.63,11,16.17,11,18c0,1.52-0.87,2.83-2.12,3.51 c0.98,0.32,2.03,0.5,3.11,0.5c3.5,0,6.58-1.8,8.37-4.52C18,17.72,13.38,16.52,11.1,12.08z" />
											</g>
											<path
												d="M7,16l-0.18,0C6.4,14.84,5.3,14,4,14c-1.66,0-3,1.34-3,3s1.34,3,3,3c0.62,0,2.49,0,3,0c1.1,0,2-0.9,2-2 C9,16.9,8.1,16,7,16z" />
										</g>
									</g>
								</svg>

							</div>
						</div>
						<svg xmlns="http://www.w3.org/2000/svg" id="toggle-menu" height="24" viewBox="0 0 24 24"
							width="24">
							<path d="M0 0h24v24H0z" fill="none" />
							<path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z" />
						</svg>

					</div>
					<div id="search">
						<input type="text" placeholder="Search... (beta)" autocomplete="off" />
					</div>
				</div>
			</div>
			<nav class="search hidden"></nav>
			<nav class="content hidden">
				<ul>
					<li class="open active">
						<div class="menu-row"><a href="./index.html">vxai</a></div>
					</li>

				</ul>
			</nav>
		</header>
		<div class="doc-scrollview" tabindex="-1" id="main-content">
			<div class="doc-container">
				<div class="doc-content">
					<section id="readme_vxai" class="doc-node">
						<div class="title">
							<h1> vxai <a href="#readme_vxai">#</a></h1>
						</div>
						<p><img src="https://github.com/ismyhc/vxai/blob/4570fab0b93d6ff48f6c440beb8e1caa88c32d05/header.png"
								alt="xAI API modue for V: vxai" /></p>
						<h1>vxai</h1>
						<p><img src="https://img.shields.io/badge/language-V-blue.svg" alt="V Language" /></p>
						<p>vxai is a V module that provides support for <a href="https://docs.x.ai/api">xAI API</a></p>
						<h2>Features</h2>
						<ul>
							<li>Simple interface for interacting with xAI API</li>
							<li>Basics tests that can be ran by providing your own API key</li>
						</ul>
						<h2>Installation</h2>
						<p>To use vxai in your V project, you can install it via VPM:</p>
						<pre><code>v install ismyhc.vxai
</code></pre>
						<p>Or clone the repository:</p>
						<pre><code>git clone https://github.com/ismyhc/vxai.git
cd vxai
v install
</code></pre>
						<h2>Usage</h2>
						<p>Take a look at the tests and the docs for more information on how to use vxai.</p>
						<h2>Todo</h2>
						<ul>
							<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox"
									disabled>Add more tests</li>
							<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox"
									disabled>Add more documentation</li>
							<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox"
									disabled>Add more examples</li>
							<li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox"
									disabled>Add streaming support.</li>
						</ul>
						<h2>License</h2>
						<p>This project is licensed under the MIT License</p>
						<h2>Contact</h2>
						<p>If you have any questions or feedback, please open an issue on this GitHub repository.</p>

					</section>

					<section id="Constants" class="doc-node const">
						<div class="title">
							<h2>Constants <a href="#Constants">#</a></h2>
						</div>


					</section>

					<section id="" class="doc-node const">
						<pre class="signature">
<code><span class="token keyword">const</span> model_path <span class="token operator">=</span> <span class="token string">'models'</span></code></pre>
						<p>The API path for models.</p>

					</section>

					<section id="" class="doc-node const">
						<pre class="signature">
<code><span class="token keyword">const</span> embedding_model_path <span class="token operator">=</span> <span class="token string">'embedding-models'</span></code></pre>
						<p>The API path for embedding models.</p>

					</section>

					<section id="" class="doc-node const">
						<pre class="signature">
<code><span class="token keyword">const</span> base_url <span class="token operator">=</span> <span class="token string">'https://api.x.ai/v1/'</span></code></pre>
						<p>base_url defines the default base URL for the X.AI API.</p>

					</section>

					<section id="" class="doc-node const">
						<pre class="signature">
<code><span class="token keyword">const</span> api_key_path <span class="token operator">=</span> <span class="token string">'api-key'</span></code></pre>
						<p>The API path for api key.</p>

					</section>

					<section id="" class="doc-node const">
						<pre class="signature">
<code><span class="token keyword">const</span> chat_completion_path <span class="token operator">=</span> <span class="token string">'chat/completions'</span></code></pre>
						<p>The API path for chat completions.</p>

					</section>

					<section id="" class="doc-node const">
						<pre class="signature">
<code><span class="token keyword">const</span> completion_path <span class="token operator">=</span> <span class="token string">'completions'</span></code></pre>
						<p>The API path for completions.</p>

					</section>

					<section id="" class="doc-node const">
						<pre class="signature">
<code><span class="token keyword">const</span> language_model_path <span class="token operator">=</span> <span class="token string">'language-models'</span></code></pre>
						<p>The API path for language models.</p>

					</section>

					<section id="get_api_key_from_env" class="doc-node">
						<div class="title">
							<h2>fn get_api_key_from_env <a href="#get_api_key_from_env">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> <span class="token function">get_api_key_from_env</span><span class="token punctuation">(</span>env <span class="token builtin">string</span><span class="token punctuation">)</span> <span class="token operator">!</span><span class="token builtin">string</span></code></pre>
						<p>get_api_key_from_env retrieves an API key from the specified environment variable.</p>
						<p>Parameters:- env: The name of the environment variable containing the API key.</p>
						<p>Returns:- The API key as a string if the environment variable is set.</p>
						<ul>
							<li>An error if the environment variable is not set or is empty.</li>
						</ul>

					</section>

					<section id="ChatCompletionInput.new" class="doc-node">
						<div class="title">
							<h2>fn ChatCompletionInput.new <a href="#ChatCompletionInput.new">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> ChatCompletionInput<span class="token punctuation">.</span><span class="token function">new</span><span class="token punctuation">(</span>messages <span class="token punctuation">[</span><span class="token punctuation">]</span>ChatCompletionMessage<span class="token punctuation">,</span> model <span class="token builtin">string</span><span class="token punctuation">)</span> ChatCompletionInput</code></pre>
						<p>ChatCompletionInput.new creates a new ChatCompletionInput with the given messages and model.
						</p>
						<p>Parameters:- messages: A list of input messages defining the conversation context.</p>
						<ul>
							<li>model: The name of the model to use for the completion.</li>
						</ul>
						<p>Returns:- A ChatCompletionInput instance with the specified parameters.</p>

					</section>

					<section id="CompletionInput.new" class="doc-node">
						<div class="title">
							<h2>fn CompletionInput.new <a href="#CompletionInput.new">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> CompletionInput<span class="token punctuation">.</span><span class="token function">new</span><span class="token punctuation">(</span>prompt <span class="token builtin">string</span><span class="token punctuation">,</span> model <span class="token builtin">string</span><span class="token punctuation">)</span> CompletionInput</code></pre>
						<p>CompletionInput.new creates a new CompletionInput instance with the given prompt and model.
						</p>
						<p>Parameters:- prompt: The input text to generate a completion for.</p>
						<ul>
							<li>model: The name of the model to use for generating the completion.</li>
						</ul>
						<p>Returns:- A new CompletionInput instance.</p>

					</section>

					<section id="XAIClient.new" class="doc-node">
						<div class="title">
							<h2>fn XAIClient.new <a href="#XAIClient.new">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> XAIClient<span class="token punctuation">.</span><span class="token function">new</span><span class="token punctuation">(</span>p XAIClientParams<span class="token punctuation">)</span> XAIClient</code></pre>
						<p>new creates a new instance of the XAIClient using the provided parameters.</p>

					</section>

					<section id="APIKeyInfo" class="doc-node">
						<div class="title">
							<h2>struct APIKeyInfo <a href="#APIKeyInfo">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">APIKeyInfo</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// acls is a list of access control rules associated with the API key.</span>
	<span class="token comment">// Examples include "api-key:model:*" or "api-key:endpoint:*".</span>
	acls <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token builtin">string</span>

	<span class="token comment">// api_key_blocked indicates whether the API key is currently blocked.</span>
	api_key_blocked <span class="token builtin">bool</span>

	<span class="token comment">// api_key_disabled indicates whether the API key is currently disabled.</span>
	api_key_disabled <span class="token builtin">bool</span>

	<span class="token comment">// api_key_id is the unique identifier for the API key.</span>
	api_key_id <span class="token builtin">string</span>

	<span class="token comment">// create_time is the ISO 8601 timestamp indicating when the API key was created.</span>
	create_time <span class="token builtin">string</span>

	<span class="token comment">// modified_by is the unique identifier of the user who last modified the API key.</span>
	modified_by <span class="token builtin">string</span>

	<span class="token comment">// modify_time is the ISO 8601 timestamp indicating when the API key was last modified.</span>
	modify_time <span class="token builtin">string</span>

	<span class="token comment">// name is the user-defined name of the API key.</span>
	name <span class="token builtin">string</span>

	<span class="token comment">// redacted_api_key is a partially hidden representation of the API key for display purposes.</span>
	redacted_api_key <span class="token builtin">string</span>

	<span class="token comment">// team_blocked indicates whether the team associated with the API key is blocked.</span>
	team_blocked <span class="token builtin">bool</span>

	<span class="token comment">// team_id is the unique identifier for the team associated with the API key.</span>
	team_id <span class="token builtin">string</span>

	<span class="token comment">// user_id is the unique identifier for the user associated with the API key.</span>
	user_id <span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>APIKeyInfo represents detailed information about an API key, including its status, associated
							user, and access controls.</p>

					</section>

					<section id="ChatCompletionChoice" class="doc-node">
						<div class="title">
							<h2>struct ChatCompletionChoice <a href="#ChatCompletionChoice">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">ChatCompletionChoice</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// finish_reason explains why the completion stopped (e.g., "length", "stop").</span>
	finish_reason <span class="token builtin">string</span>

	<span class="token comment">// index is the position of this choice in the list of returned choices.</span>
	index <span class="token builtin">int</span>

	<span class="token comment">// message contains the content of the generated message.</span>
	message ChatCompletionMessage
<span class="token punctuation">}</span></code></pre>
						<p>ChatCompletionChoice represents a single completion choice returned by the API.</p>

					</section>

					<section id="ChatCompletionInput" class="doc-node">
						<div class="title">
							<h2>struct ChatCompletionInput <a href="#ChatCompletionInput">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">ChatCompletionInput</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// Number between -2.0 and 2.0. Positive values penalize new tokens based on</span>
	<span class="token comment">// their existing frequency in the text so far, decreasing the model's</span>
	<span class="token comment">// likelihood to repeat the same line verbatim.</span>
	frequency_penalty <span class="token operator">?</span><span class="token builtin">f32</span>

	<span class="token comment">// A JSON object that maps tokens (specified by their token ID in the tokenizer)</span>
	<span class="token comment">// to an associated bias value from -100 to 100. Mathematically, the bias is added</span>
	<span class="token comment">// to the logits generated by the model prior to sampling. The exact effect will</span>
	<span class="token comment">// vary per model, but values between -1 and 1 should decrease or increase likelihood</span>
	<span class="token comment">// of selection; values like -100 or 100 should result in a ban or exclusive selection</span>
	<span class="token comment">//  of the relevant token.</span>
	logit_bias <span class="token operator">?</span><span class="token builtin">map</span><span class="token punctuation">[</span><span class="token builtin">string</span><span class="token punctuation">]</span><span class="token builtin">f32</span>

	<span class="token comment">// Whether to return log probabilities of the output tokens or not. If true, returns</span>
	<span class="token comment">// the log probabilities of each output token returned in the content of message.</span>
	logprobs <span class="token operator">?</span><span class="token builtin">bool</span>

	<span class="token comment">// The maximum number of tokens that can be generated in the chat completion. This</span>
	<span class="token comment">// value can be used to control costs for text generated via API.</span>
	max_tokens <span class="token operator">?</span><span class="token builtin">int</span>

	<span class="token comment">// A list of messages that make up the the chat conversation. Different models</span>
	<span class="token comment">// support different message types, such as image and text.</span>
	messages <span class="token punctuation">[</span><span class="token punctuation">]</span>ChatCompletionMessage

	<span class="token comment">// Model name for the model to use.</span>
	model <span class="token builtin">string</span>

	<span class="token comment">// How many chat completion choices to generate for each input message. Note that</span>
	<span class="token comment">// you will be charged based on the number of generated tokens across all of the</span>
	<span class="token comment">// choices. Keep n as 1 to minimize costs.</span>
	n <span class="token operator">?</span><span class="token builtin">int</span>

	<span class="token comment">// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether</span>
	<span class="token comment">// they appear in the text so far, increasing the model's likelihood to talk about new topics.</span>
	presence_penalty <span class="token operator">?</span><span class="token builtin">f32</span>

	<span class="token comment">// response_format specifies the format of the response, such as plain text or JSON.</span>
	<span class="token comment">// When null, the default format is used.</span>
	response_format <span class="token operator">?</span><span class="token builtin">string</span>

	<span class="token comment">// If specified, our system will make a best effort to sample deterministically, such that</span>
	<span class="token comment">// repeated requests with the same `seed` and parameters should return the same result.</span>
	<span class="token comment">// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response</span>
	<span class="token comment">// parameter to monitor changes in the backend.</span>
	seed <span class="token operator">?</span><span class="token builtin">int</span>

	<span class="token comment">// Up to 4 sequences where the API will stop generating further tokens.</span>
	stop <span class="token operator">?</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token builtin">string</span>
<span class="token keyword">pub</span> <span class="token keyword">mut</span><span class="token punctuation">:</span>
	<span class="token comment">// If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent</span>
	<span class="token comment">// events as they become available, with the stream terminated by a `data: [DONE]` message.</span>
	stream <span class="token builtin">bool</span>

	<span class="token comment">// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the</span>
	<span class="token comment">// output more random, while lower values like 0.2 will make it more focused and deterministic.</span>
	temperature <span class="token operator">?</span><span class="token builtin">f32</span>

	<span class="token comment">// A list of tools the model may call. Currently, only functions are supported as a tool. Use</span>
	<span class="token comment">// this to provide a list of functions the model may generate JSON inputs for. A max of 128</span>
	<span class="token comment">// functions are supported.</span>
	tool_choice <span class="token operator">?</span><span class="token builtin">string</span>
	tools       <span class="token operator">?</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token builtin">string</span>

	<span class="token comment">// An integer between 0 and 20 specifying the number of most likely tokens to return at each</span>
	<span class="token comment">// token position, each with an associated log probability. logprobs must be set to true if</span>
	<span class="token comment">// this parameter is used.</span>
	top_logprobs <span class="token operator">?</span><span class="token builtin">int</span>

	<span class="token comment">// An alternative to sampling with temperature, called nucleus sampling, where the model</span>
	<span class="token comment">// considers the results of the tokens with top_p probability mass. So 0.1 means only the</span>
	<span class="token comment">// tokens comprising the top 10% probability mass are considered. It is generally recommended</span>
	<span class="token comment">// to alter this or `temperature` but not both.</span>
	top_p <span class="token operator">?</span><span class="token builtin">f32</span>

	<span class="token comment">// A unique identifier representing your end-user, which can help xAI to monitor and detect abuse</span>
	user <span class="token operator">?</span><span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>ChatCompletionInput represents the input parameters for a chat completion request. It
							includes options for customizing the behavior and output of the model.</p>

					</section>

					<section id="ChatCompletionMessage" class="doc-node">
						<div class="title">
							<h2>struct ChatCompletionMessage <a href="#ChatCompletionMessage">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">ChatCompletionMessage</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// role is the role of the message sender (e.g., "user", "assistant", "system").</span>
	role <span class="token builtin">string</span>

	<span class="token comment">// content is the text content of the message.</span>
	content <span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>ChatCompletionMessage represents a single message in the chat conversation.</p>

					</section>

					<section id="ChatCompletionResponse" class="doc-node">
						<div class="title">
							<h2>struct ChatCompletionResponse <a href="#ChatCompletionResponse">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">ChatCompletionResponse</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// choices contains the list of generated messages.</span>
	choices <span class="token punctuation">[</span><span class="token punctuation">]</span>ChatCompletionChoice

	<span class="token comment">// created is the Unix timestamp when the response was generated.</span>
	created <span class="token builtin">i64</span>

	<span class="token comment">// id is the unique identifier for this API response.</span>
	id <span class="token builtin">string</span>

	<span class="token comment">// model specifies the model used for this completion.</span>
	model <span class="token builtin">string</span>

	<span class="token comment">// object specifies the type of object returned (e.g., "text_completion").</span>
	object <span class="token builtin">string</span>

	<span class="token comment">// system_fingerprint provides a system-level tracking identifier.</span>
	system_fingerprint <span class="token builtin">string</span>

	<span class="token comment">// usage provides details about token usage in the request and response.</span>
	usage ChatCompletionUsage
<span class="token punctuation">}</span></code></pre>
						<p>ChatCompletionResponse represents the full response from the chat completion API.</p>

					</section>

					<section id="ChatCompletionUsage" class="doc-node">
						<div class="title">
							<h2>struct ChatCompletionUsage <a href="#ChatCompletionUsage">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">ChatCompletionUsage</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// completion_tokens is the number of tokens in the generated response.</span>
	completion_tokens <span class="token builtin">int</span>

	<span class="token comment">// prompt_tokens is the number of tokens in the input prompt.</span>
	prompt_tokens <span class="token builtin">int</span>

	<span class="token comment">// total_tokens is the total number of tokens used (prompt + completion).</span>
	total_tokens <span class="token builtin">int</span>
<span class="token punctuation">}</span></code></pre>
						<p>ChatCompletionUsage provides details about token usage for a request and response.</p>

					</section>

					<section id="CompletionChoice" class="doc-node">
						<div class="title">
							<h2>struct CompletionChoice <a href="#CompletionChoice">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">CompletionChoice</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// finish_reason indicates why the generation stopped (e.g., "length", "stop").</span>
	finish_reason <span class="token builtin">string</span>

	<span class="token comment">// index is the position of this choice in the list of returned choices.</span>
	index <span class="token builtin">int</span>

	<span class="token comment">// text contains the generated text for this completion.</span>
	text <span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>CompletionChoice represents an individual generated completion within the response.</p>

					</section>

					<section id="CompletionInput" class="doc-node">
						<div class="title">
							<h2>struct CompletionInput <a href="#CompletionInput">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">CompletionInput</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// The prompt to generate completions for.</span>
	prompt <span class="token builtin">string</span>

	<span class="token comment">// Number between -2.0 and 2.0. Positive values penalize new tokens based on</span>
	<span class="token comment">// their existing frequency in the text so far, decreasing the model's</span>
	<span class="token comment">// likelihood to repeat the same line verbatim.</span>
	frequency_penalty <span class="token operator">?</span><span class="token builtin">f32</span>

	<span class="token comment">// A JSON object that maps tokens (specified by their token ID in the tokenizer)</span>
	<span class="token comment">// to an associated bias value from -100 to 100. Mathematically, the bias is added</span>
	<span class="token comment">// to the logits generated by the model prior to sampling. The exact effect will</span>
	<span class="token comment">// vary per model, but values between -1 and 1 should decrease or increase likelihood</span>
	<span class="token comment">// of selection; values like -100 or 100 should result in a ban or exclusive selection</span>
	<span class="token comment">//  of the relevant token.</span>
	logit_bias <span class="token operator">?</span><span class="token builtin">map</span><span class="token punctuation">[</span><span class="token builtin">string</span><span class="token punctuation">]</span><span class="token builtin">f32</span>

	<span class="token comment">// Whether to return log probabilities of the output tokens or not. If true, returns</span>
	<span class="token comment">// the log probabilities of each output token returned in the content of message.</span>
	logprobs <span class="token operator">?</span><span class="token builtin">bool</span>

	<span class="token comment">// The maximum number of tokens that can be generated in the chat completion. This</span>
	<span class="token comment">// value can be used to control costs for text generated via API.</span>
	max_tokens <span class="token operator">?</span><span class="token builtin">int</span>

	<span class="token comment">// Specifies the model to be used for the request.</span>
	model <span class="token builtin">string</span>

	<span class="token comment">// How many chat completion choices to generate for each input message. Note that</span>
	<span class="token comment">// you will be charged based on the number of generated tokens across all of the</span>
	<span class="token comment">// choices. Keep n as 1 to minimize costs.</span>
	n <span class="token operator">?</span><span class="token builtin">int</span>

	<span class="token comment">// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether</span>
	<span class="token comment">// they appear in the text so far, increasing the model's likelihood to talk about new topics.</span>
	presence_penalty <span class="token operator">?</span><span class="token builtin">f32</span>

	<span class="token comment">// response_format specifies the format of the response, such as plain text or JSON.</span>
	<span class="token comment">// When null, the default format is used.</span>
	response_format <span class="token operator">?</span><span class="token builtin">string</span>

	<span class="token comment">// If specified, our system will make a best effort to sample deterministically, such that</span>
	<span class="token comment">// repeated requests with the same `seed` and parameters should return the same result.</span>
	<span class="token comment">// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response</span>
	<span class="token comment">// parameter to monitor changes in the backend.</span>
	seed <span class="token operator">?</span><span class="token builtin">int</span>

	<span class="token comment">// Up to 4 sequences where the API will stop generating further tokens.</span>
	stop <span class="token operator">?</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token builtin">string</span>
<span class="token keyword">pub</span> <span class="token keyword">mut</span><span class="token punctuation">:</span>
	<span class="token comment">// If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent</span>
	<span class="token comment">// events as they become available, with the stream terminated by a `data: [DONE]` message.</span>
	stream <span class="token builtin">bool</span>

	<span class="token comment">// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the</span>
	<span class="token comment">// output more random, while lower values like 0.2 will make it more focused and deterministic.</span>
	temperature <span class="token operator">?</span><span class="token builtin">f32</span>

	<span class="token comment">// An alternative to sampling with temperature, called nucleus sampling, where the model</span>
	<span class="token comment">// considers the results of the tokens with top_p probability mass. So 0.1 means only the</span>
	<span class="token comment">// tokens comprising the top 10% probability mass are considered. It is generally recommended</span>
	<span class="token comment">// to alter this or `temperature` but not both.</span>
	top_p <span class="token operator">?</span><span class="token builtin">f32</span>

	<span class="token comment">// A unique identifier representing your end-user, which can help xAI to monitor and detect abuse</span>
	user <span class="token operator">?</span><span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>CompletionInput represents the input parameters for a completion request. It includes options
							for customizing the behavior and output of the model.</p>

					</section>

					<section id="CompletionResponse" class="doc-node">
						<div class="title">
							<h2>struct CompletionResponse <a href="#CompletionResponse">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">CompletionResponse</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// choices contains the list of generated completions.</span>
	choices <span class="token punctuation">[</span><span class="token punctuation">]</span>CompletionChoice

	<span class="token comment">// created is the Unix timestamp indicating when the response was generated.</span>
	created <span class="token builtin">i64</span>

	<span class="token comment">// id is a unique identifier for this completion response.</span>
	id <span class="token builtin">string</span>

	<span class="token comment">// model specifies the model used to generate the completion.</span>
	model <span class="token builtin">string</span>

	<span class="token comment">// object specifies the type of object returned, typically "text_completion".</span>
	object <span class="token builtin">string</span>

	<span class="token comment">// system_fingerprint is a server-generated identifier for debugging or tracking.</span>
	system_fingerprint <span class="token builtin">string</span>

	<span class="token comment">// usage provides token usage details, including prompt and completion token counts.</span>
	usage CompletionUsage
<span class="token punctuation">}</span></code></pre>
						<p>CompletionResponse represents the response from a text completion API call. It includes the
							generated choices, metadata about the request, and token usage details.</p>

					</section>

					<section id="CompletionUsage" class="doc-node">
						<div class="title">
							<h2>struct CompletionUsage <a href="#CompletionUsage">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">CompletionUsage</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// completion_tokens is the number of tokens in the generated completion.</span>
	completion_tokens <span class="token builtin">int</span>

	<span class="token comment">// prompt_tokens is the number of tokens in the input prompt.</span>
	prompt_tokens <span class="token builtin">int</span>

	<span class="token comment">// total_tokens is the total number of tokens processed (prompt + completion).</span>
	total_tokens <span class="token builtin">int</span>
<span class="token punctuation">}</span></code></pre>
						<p>CompletionUsage provides statistics on token usage for a completion request.</p>

					</section>

					<section id="EmbeddingModel" class="doc-node">
						<div class="title">
							<h2>struct EmbeddingModel <a href="#EmbeddingModel">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">EmbeddingModel</span> <span class="token punctuation">{</span>
	<span class="token comment">// created is the Unix timestamp indicating when the model was created.</span>
	created <span class="token builtin">i64</span>

	<span class="token comment">// id is the unique identifier for the embedding model.</span>
	id <span class="token builtin">string</span>

	<span class="token comment">// input_modalities is a list of supported input types for the model, such as "text" or "image".</span>
	input_modalities <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token builtin">string</span>

	<span class="token comment">// object specifies the type of object represented, typically "model".</span>
	object <span class="token builtin">string</span>

	<span class="token comment">// owned_by indicates the owner of the model, usually the organization or team responsible for it.</span>
	owned_by <span class="token builtin">string</span>

	<span class="token comment">// prompt_image_token_price specifies the token price for image-based inputs.</span>
	prompt_image_token_price <span class="token builtin">i64</span>

	<span class="token comment">// prompt_text_token_price specifies the token price for text-based inputs.</span>
	prompt_text_token_price <span class="token builtin">i64</span>

	<span class="token comment">// version specifies the version of the embedding model, such as "1.0.0".</span>
	version <span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>EmbeddingModel represents a specific embedding model available in the X.AI API. It includes
							details about the model's ID, creation time, input capabilities, and pricing.</p>

					</section>

					<section id="EmbeddingModelsResponse" class="doc-node">
						<div class="title">
							<h2>struct EmbeddingModelsResponse <a href="#EmbeddingModelsResponse">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">EmbeddingModelsResponse</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// models is the list of embedding models available in the API.</span>
	models <span class="token punctuation">[</span><span class="token punctuation">]</span>EmbeddingModel
<span class="token punctuation">}</span></code></pre>
						<p>EmbeddingModelsResponse represents the response from the API when querying for all available
							embedding models. It contains a list of EmbeddingModel objects.</p>

					</section>

					<section id="LanguageModel" class="doc-node">
						<div class="title">
							<h2>struct LanguageModel <a href="#LanguageModel">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">LanguageModel</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// completion_text_token_price specifies the token price for text completions generated by the model.</span>
	completion_text_token_price <span class="token builtin">i64</span>

	<span class="token comment">// created is the Unix timestamp indicating when the model was created.</span>
	created <span class="token builtin">i64</span>

	<span class="token comment">// id is the unique identifier for the language model.</span>
	id <span class="token builtin">string</span>

	<span class="token comment">// input_modalities lists the types of input supported by the model, such as "text" or "image".</span>
	input_modalities <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token builtin">string</span>

	<span class="token comment">// object specifies the type of object represented, typically "model".</span>
	object <span class="token builtin">string</span>

	<span class="token comment">// output_modalities lists the types of output that the model can generate, such as "text".</span>
	output_modalities <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token builtin">string</span>

	<span class="token comment">// owned_by indicates the owner of the model, usually the organization or team responsible for it.</span>
	owned_by <span class="token builtin">string</span>

	<span class="token comment">// prompt_image_token_price specifies the token price for image-based inputs.</span>
	prompt_image_token_price <span class="token builtin">i64</span>

	<span class="token comment">// prompt_text_token_price specifies the token price for text-based inputs.</span>
	prompt_text_token_price <span class="token builtin">i64</span>
<span class="token punctuation">}</span></code></pre>
						<p>LanguageModel represents a specific language model available in the X.AI API. It includes
							details about the model's ID, creation time, input/output capabilities, and pricing.</p>

					</section>

					<section id="LanguageModelsResponse" class="doc-node">
						<div class="title">
							<h2>struct LanguageModelsResponse <a href="#LanguageModelsResponse">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">LanguageModelsResponse</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// models is the list of language models available in the API.</span>
	models <span class="token punctuation">[</span><span class="token punctuation">]</span>LanguageModel
<span class="token punctuation">}</span></code></pre>
						<p>LanguageModelsResponse represents the response from the API when querying for all available
							language models. It contains a list of LanguageModel objects.</p>

					</section>

					<section id="Model" class="doc-node">
						<div class="title">
							<h2>struct Model <a href="#Model">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">Model</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// created is the Unix timestamp indicating when the model was created.</span>
	created <span class="token builtin">i64</span>

	<span class="token comment">// id is the unique identifier for the model.</span>
	id <span class="token builtin">string</span>

	<span class="token comment">// object specifies the type of object represented, typically "model".</span>
	object <span class="token builtin">string</span>

	<span class="token comment">// owned_by indicates the owner of the model, usually the organization or team responsible for it.</span>
	owned_by <span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>Model represents a model in the X.AI API. It includes details such as the creation time, ID,
							ownership, and object type.</p>

					</section>

					<section id="ModelsResponse" class="doc-node">
						<div class="title">
							<h2>struct ModelsResponse <a href="#ModelsResponse">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">ModelsResponse</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// models is the list of models available in the API.</span>
	models <span class="token punctuation">[</span><span class="token punctuation">]</span>Model @<span class="token punctuation">[</span>json<span class="token punctuation">:</span> data<span class="token punctuation">]</span>
<span class="token punctuation">}</span></code></pre>
						<p>ModelsResponse represents the response from the API when querying for all models. It contains
							a list of Model objects.</p>
						<p>The <code>@json: data</code> attribute maps the "data" key in the JSON response to the
							<code>models</code> field.</p>

					</section>

					<section id="StreamChatCompletionChoice" class="doc-node">
						<div class="title">
							<h2>struct StreamChatCompletionChoice <a href="#StreamChatCompletionChoice">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">StreamChatCompletionChoice</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// index is the position of this choice in the list of streamed choices.</span>
	index <span class="token builtin">int</span>

	<span class="token comment">// delta contains the incremental message content for this choice.</span>
	delta StreamChatCompletionDelta

	<span class="token comment">// finish_reason explains why this choice's generation stopped (if applicable).</span>
	finish_reason <span class="token operator">?</span><span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>StreamChatCompletionChoice represents a single partial completion returned during streaming.
						</p>

					</section>

					<section id="StreamChatCompletionChunk" class="doc-node">
						<div class="title">
							<h2>struct StreamChatCompletionChunk <a href="#StreamChatCompletionChunk">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">StreamChatCompletionChunk</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// id is the unique identifier for this chunk.</span>
	id <span class="token builtin">string</span>

	<span class="token comment">// object specifies the type of object returned.</span>
	object <span class="token builtin">string</span>

	<span class="token comment">// created is the Unix timestamp when this chunk was generated.</span>
	created <span class="token builtin">i64</span>

	<span class="token comment">// choices contains the partial completion choices in this chunk.</span>
	choices <span class="token punctuation">[</span><span class="token punctuation">]</span>StreamChatCompletionChoice

	<span class="token comment">// usage provides token usage information for this chunk.</span>
	usage StreamChatCompletionUsage

	<span class="token comment">// system_fingerprint provides a system-level tracking identifier for this chunk.</span>
	system_fingerprint <span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>StreamChatCompletionChunk represents a single chunk of a streamed response.</p>

					</section>

					<section id="StreamChatCompletionDelta" class="doc-node">
						<div class="title">
							<h2>struct StreamChatCompletionDelta <a href="#StreamChatCompletionDelta">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">StreamChatCompletionDelta</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// content is the partial message content for this update.</span>
	content <span class="token operator">?</span><span class="token builtin">string</span>

	<span class="token comment">// role is the role of the message sender for this update.</span>
	role <span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>StreamChatCompletionDelta represents an incremental update to a message during streaming.</p>

					</section>

					<section id="StreamChatCompletionPromptTokenDetails" class="doc-node">
						<div class="title">
							<h2>struct StreamChatCompletionPromptTokenDetails <a
									href="#StreamChatCompletionPromptTokenDetails">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">StreamChatCompletionPromptTokenDetails</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// text_tokens is the number of tokens from text inputs.</span>
	text_tokens <span class="token builtin">int</span>

	<span class="token comment">// audio_tokens is the number of tokens from audio inputs.</span>
	audio_tokens <span class="token builtin">int</span>

	<span class="token comment">// image_tokens is the number of tokens from image inputs.</span>
	image_tokens <span class="token builtin">int</span>

	<span class="token comment">// cached_tokens is the number of tokens retrieved from cache.</span>
	cached_tokens <span class="token builtin">int</span>
<span class="token punctuation">}</span></code></pre>
						<p>StreamChatCompletionPromptTokenDetails provides a breakdown of tokens in the input prompt.
						</p>

					</section>

					<section id="StreamChatCompletionUsage" class="doc-node">
						<div class="title">
							<h2>struct StreamChatCompletionUsage <a href="#StreamChatCompletionUsage">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">StreamChatCompletionUsage</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// prompt_tokens is the number of tokens in the input prompt.</span>
	prompt_tokens <span class="token builtin">int</span>

	<span class="token comment">// completion_tokens is the number of tokens generated so far.</span>
	completion_tokens <span class="token builtin">int</span>

	<span class="token comment">// total_tokens is the total number of tokens used (prompt + completion).</span>
	total_tokens <span class="token builtin">int</span>

	<span class="token comment">// prompt_tokens_details provides a detailed breakdown of the input tokens.</span>
	prompt_tokens_details StreamChatCompletionPromptTokenDetails
<span class="token punctuation">}</span></code></pre>
						<p>StreamChatCompletionUsage provides token usage details for a streaming request.</p>

					</section>

					<section id="StreamCompletionChoice" class="doc-node">
						<div class="title">
							<h2>struct StreamCompletionChoice <a href="#StreamCompletionChoice">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">StreamCompletionChoice</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// index is the position of this choice in the list of returned choices.</span>
	index <span class="token builtin">int</span>

	<span class="token comment">// delta contains newly streamed content or role information for this choice.</span>
	delta StreamCompletionDelta

	<span class="token comment">// finish_reason is an optional reason indicating why this choice concluded (e.g., "stop").</span>
	finish_reason <span class="token operator">?</span><span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>StreamCompletionChoice represents an individual choice within a streamed chunk.</p>

					</section>

					<section id="StreamCompletionChunk" class="doc-node">
						<div class="title">
							<h2>struct StreamCompletionChunk <a href="#StreamCompletionChunk">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">StreamCompletionChunk</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// id is a unique identifier for this specific chunk of the completion.</span>
	id <span class="token builtin">string</span>

	<span class="token comment">// object specifies the type of the returned object, typically "completion.chunk".</span>
	object <span class="token builtin">string</span>

	<span class="token comment">// created is the Unix timestamp (in seconds) indicating when this chunk was generated.</span>
	created <span class="token builtin">i64</span>

	<span class="token comment">// choices is an array of StreamCompletionChoice objects, each representing a portion of the streamed completion.</span>
	choices <span class="token punctuation">[</span><span class="token punctuation">]</span>StreamCompletionChoice

	<span class="token comment">// usage contains token usage statistics for this chunk, including prompt and completion tokens.</span>
	usage StreamCompletionUsage

	<span class="token comment">// system_fingerprint is a server-generated identifier for debugging or reference.</span>
	system_fingerprint <span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>StreamCompletionChunk represents a single streamed chunk of a completion response.</p>

					</section>

					<section id="StreamCompletionDelta" class="doc-node">
						<div class="title">
							<h2>struct StreamCompletionDelta <a href="#StreamCompletionDelta">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">StreamCompletionDelta</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// content is the newly generated text or partial completion content, if any.</span>
	content <span class="token operator">?</span><span class="token builtin">string</span>

	<span class="token comment">// role is the role associated with this content, such as "assistant" or "system".</span>
	role <span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>StreamCompletionDelta represents incremental updates to the streamed completion.</p>

					</section>

					<section id="StreamCompletionPromptTokenDetails" class="doc-node">
						<div class="title">
							<h2>struct StreamCompletionPromptTokenDetails <a
									href="#StreamCompletionPromptTokenDetails">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">StreamCompletionPromptTokenDetails</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// text_tokens is the number of tokens derived from textual input.</span>
	text_tokens <span class="token builtin">int</span>

	<span class="token comment">// audio_tokens is the number of tokens derived from audio input.</span>
	audio_tokens <span class="token builtin">int</span>

	<span class="token comment">// image_tokens is the number of tokens derived from image input.</span>
	image_tokens <span class="token builtin">int</span>

	<span class="token comment">// cached_tokens is the number of tokens retrieved from cache.</span>
	cached_tokens <span class="token builtin">int</span>
<span class="token punctuation">}</span></code></pre>
						<p>StreamCompletionPromptTokenDetails provides a breakdown of the prompt tokens by type.</p>

					</section>

					<section id="StreamCompletionUsage" class="doc-node">
						<div class="title">
							<h2>struct StreamCompletionUsage <a href="#StreamCompletionUsage">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">StreamCompletionUsage</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// prompt_tokens is the number of tokens used in the input prompt.</span>
	prompt_tokens <span class="token builtin">int</span>

	<span class="token comment">// completion_tokens is the number of tokens generated in this streamed response.</span>
	completion_tokens <span class="token builtin">int</span>

	<span class="token comment">// total_tokens is the total number of tokens processed so far (prompt + completion).</span>
	total_tokens <span class="token builtin">int</span>

	<span class="token comment">// prompt_tokens_details provides a detailed breakdown of the input tokens by type (e.g., text, audio).</span>
	prompt_tokens_details StreamCompletionPromptTokenDetails
<span class="token punctuation">}</span></code></pre>
						<p>StreamCompletionUsage contains token usage details for a streamed completion segment.</p>

					</section>

					<section id="XAIClient" class="doc-node">
						<div class="title">
							<h2>struct XAIClient <a href="#XAIClient">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">XAIClient</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// api_key is the API key used for authentication.</span>
	api_key <span class="token builtin">string</span>

	<span class="token comment">// base_url is the base URL for API requests.</span>
	<span class="token comment">// Defaults to `https://api.x.ai/v1/` but can be customized.</span>
	base_url <span class="token builtin">string</span>
<span class="token punctuation">}</span></code></pre>
						<p>XAIClient is a client for interacting with the X.AI API. It provides methods for making
							authenticated GET and POST requests.</p>

					</section>

					<section id="XAIClient.get_api_key_info" class="doc-node">
						<div class="title">
							<h2>fn (XAIClient) get_api_key_info <a href="#XAIClient.get_api_key_info">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> <span class="token punctuation">(</span>c XAIClient<span class="token punctuation">)</span> <span class="token function">get_api_key_info</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!</span>APIKeyInfo</code></pre>
						<p>get_api_key_info retrieves information about the current API key being used by the client. It
							sends a GET request to the "api-key" endpoint and parses the response into an APIKeyInfo
							struct.</p>
						<p>Returns:- An APIKeyInfo struct containing detailed API key information if successful.</p>
						<ul>
							<li>An error if the request or JSON decoding fails.</li>
						</ul>

					</section>

					<section id="XAIClient.get_chat_completion" class="doc-node">
						<div class="title">
							<h2>fn (XAIClient) get_chat_completion <a href="#XAIClient.get_chat_completion">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> <span class="token punctuation">(</span>c XAIClient<span class="token punctuation">)</span> <span class="token function">get_chat_completion</span><span class="token punctuation">(</span>input ChatCompletionInput<span class="token punctuation">)</span> <span class="token operator">!</span>ChatCompletionResponse</code></pre>
						<p>get_chat_completion sends a chat completion request to the API.</p>
						<p>Parameters:- input: A ChatCompletionInput instance containing the request parameters.</p>
						<p>Returns:- A ChatCompletionResponse containing the generated completions.</p>
						<ul>
							<li>An error if the request fails or if the response cannot be decoded.</li>
						</ul>

					</section>

					<section id="XAIClient.get_completion" class="doc-node">
						<div class="title">
							<h2>fn (XAIClient) get_completion <a href="#XAIClient.get_completion">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> <span class="token punctuation">(</span>c XAIClient<span class="token punctuation">)</span> <span class="token function">get_completion</span><span class="token punctuation">(</span>input CompletionInput<span class="token punctuation">)</span> <span class="token operator">!</span>CompletionResponse</code></pre>
						<p>get_completion sends a completion request to the API.</p>
						<p>Parameters:- input: A CompletionInput instance containing the request parameters.</p>
						<p>Returns:- A CompletionResponse containing the generated completions.</p>
						<ul>
							<li>An error if the request fails or if the response cannot be decoded.</li>
						</ul>

					</section>

					<section id="XAIClient.get_embedding_model" class="doc-node">
						<div class="title">
							<h2>fn (XAIClient) get_embedding_model <a href="#XAIClient.get_embedding_model">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> <span class="token punctuation">(</span>c XAIClient<span class="token punctuation">)</span> <span class="token function">get_embedding_model</span><span class="token punctuation">(</span>id <span class="token builtin">string</span><span class="token punctuation">)</span> <span class="token operator">!</span>EmbeddingModel</code></pre>
						<p>get_embedding_model retrieves details about a specific embedding model by its ID.</p>
						<p>Parameters:- id: The unique identifier of the embedding model to retrieve.</p>
						<p>Returns:- An EmbeddingModel struct containing details about the requested model.</p>
						<ul>
							<li>An error if the request fails or if the response cannot be decoded.</li>
						</ul>

					</section>

					<section id="XAIClient.get_embedding_models" class="doc-node">
						<div class="title">
							<h2>fn (XAIClient) get_embedding_models <a href="#XAIClient.get_embedding_models">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> <span class="token punctuation">(</span>c XAIClient<span class="token punctuation">)</span> <span class="token function">get_embedding_models</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!</span>EmbeddingModelsResponse</code></pre>
						<p>get_embedding_models retrieves all available embedding models from the X.AI API.</p>
						<p>Returns:- An EmbeddingModelsResponse struct containing a list of all available embedding
							models.</p>
						<ul>
							<li>An error if the request fails or if the response cannot be decoded.</li>
						</ul>

					</section>

					<section id="XAIClient.get_language_model" class="doc-node">
						<div class="title">
							<h2>fn (XAIClient) get_language_model <a href="#XAIClient.get_language_model">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> <span class="token punctuation">(</span>c XAIClient<span class="token punctuation">)</span> <span class="token function">get_language_model</span><span class="token punctuation">(</span>id <span class="token builtin">string</span><span class="token punctuation">)</span> <span class="token operator">!</span>LanguageModel</code></pre>
						<p>get_language_model retrieves details about a specific language model by its ID.</p>
						<p>Parameters:- id: The unique identifier of the language model to retrieve.</p>
						<p>Returns:- A LanguageModel struct containing details about the requested model.</p>
						<ul>
							<li>An error if the request fails or if the response cannot be decoded.</li>
						</ul>

					</section>

					<section id="XAIClient.get_language_models" class="doc-node">
						<div class="title">
							<h2>fn (XAIClient) get_language_models <a href="#XAIClient.get_language_models">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> <span class="token punctuation">(</span>c XAIClient<span class="token punctuation">)</span> <span class="token function">get_language_models</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!</span>LanguageModelsResponse</code></pre>
						<p>get_language_models retrieves all available language models from the X.AI API.</p>
						<p>Returns:- A LanguageModelsResponse struct containing a list of all available language models.
						</p>
						<ul>
							<li>An error if the request fails or if the response cannot be decoded.</li>
						</ul>

					</section>

					<section id="XAIClient.get_model" class="doc-node">
						<div class="title">
							<h2>fn (XAIClient) get_model <a href="#XAIClient.get_model">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> <span class="token punctuation">(</span>c XAIClient<span class="token punctuation">)</span> <span class="token function">get_model</span><span class="token punctuation">(</span>id <span class="token builtin">string</span><span class="token punctuation">)</span> <span class="token operator">!</span>Model</code></pre>
						<p>get_model retrieves details about a specific model by its ID.</p>
						<p>Parameters:- id: The unique identifier of the model to retrieve.</p>
						<p>Returns:- A Model struct containing details about the requested model.</p>
						<ul>
							<li>An error if the request fails or if the response cannot be decoded.</li>
						</ul>

					</section>

					<section id="XAIClient.get_models" class="doc-node">
						<div class="title">
							<h2>fn (XAIClient) get_models <a href="#XAIClient.get_models">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> <span class="token punctuation">(</span>c XAIClient<span class="token punctuation">)</span> <span class="token function">get_models</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!</span>ModelsResponse</code></pre>
						<p>get_models retrieves all available models from the X.AI API.</p>
						<p>Returns:- A ModelsResponse struct containing a list of all available models.</p>
						<ul>
							<li>An error if the request fails or if the response cannot be decoded.</li>
						</ul>

					</section>

					<section id="XAIClient.stream_chat_completion" class="doc-node">
						<div class="title">
							<h2>fn (XAIClient) stream_chat_completion <a href="#XAIClient.stream_chat_completion">#</a>
							</h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> <span class="token punctuation">(</span>c XAIClient<span class="token punctuation">)</span> <span class="token function">stream_chat_completion</span><span class="token punctuation">(</span><span class="token keyword">mut</span> input ChatCompletionInput<span class="token punctuation">,</span> on_message <span class="token keyword">fn</span> <span class="token punctuation">(</span>StreamOnMessageFn<span class="token punctuation">)</span><span class="token punctuation">,</span> on_finish <span class="token keyword">fn</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">!</span>http<span class="token punctuation">.</span>Response</code></pre>
						<p>stream_chat_completion sends a streaming chat completion request to the API.</p>
						<p>Parameters:- input: A mutable ChatCompletionInput instance containing the request parameters.
						</p>
						<ul>
							<li>on_message: A callback function invoked for each streamed chunk.</li>
							<li>on_finish: A callback function invoked when streaming finishes.</li>
						</ul>
						<p>Returns:- The HTTP response object for the streaming request.</p>
						<ul>
							<li>An error if the request fails.</li>
						</ul>

					</section>

					<section id="XAIClient.stream_completion" class="doc-node">
						<div class="title">
							<h2>fn (XAIClient) stream_completion <a href="#XAIClient.stream_completion">#</a></h2>
						</div>
						<pre class="signature">
<code><span class="token keyword">fn</span> <span class="token punctuation">(</span>c XAIClient<span class="token punctuation">)</span> <span class="token function">stream_completion</span><span class="token punctuation">(</span><span class="token keyword">mut</span> input CompletionInput<span class="token punctuation">,</span> on_message <span class="token keyword">fn</span> <span class="token punctuation">(</span>StreamOnMessageFn<span class="token punctuation">)</span><span class="token punctuation">,</span> on_finish <span class="token keyword">fn</span> <span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">!</span>http<span class="token punctuation">.</span>Response</code></pre>
						<p>stream_completion sends a streaming completion request to the API.</p>
						<p>Parameters:- input: A mutable CompletionInput instance containing the request parameters.</p>
						<ul>
							<li>on_message: A callback function invoked for each streamed chunk of the completion.</li>
							<li>on_finish: A callback function invoked when streaming finishes.</li>
						</ul>
						<p>Returns:- An HTTP response object for the streaming request.</p>
						<ul>
							<li>An error if the request fails.</li>
						</ul>

					</section>

					<section id="XAIClientParams" class="doc-node">
						<div class="title">
							<h2>struct XAIClientParams <a href="#XAIClientParams">#</a></h2>
						</div>
						<div class="attributes">
							<div class="attribute">@[params]</div>
						</div>
						<pre class="signature">
<code><span class="token keyword">struct</span> <span class="token symbol">XAIClientParams</span> <span class="token punctuation">{</span>
<span class="token keyword">pub</span><span class="token punctuation">:</span>
	<span class="token comment">// api_key is the API key required for authentication.</span>
	api_key <span class="token builtin">string</span>

	<span class="token comment">// base_url is the base URL for API requests.</span>
	<span class="token comment">// If not provided, it defaults to `https://api.x.ai/v1/`.</span>
	base_url <span class="token builtin">string</span> <span class="token operator">=</span> vxai<span class="token punctuation">.</span>base_url
<span class="token punctuation">}</span></code></pre>
						<p>XAIClientParams is used to initialize an XAIClient instance. It supports optional
							customization of the base URL.</p>

					</section>


					<div class="footer">Powered by vdoc. Generated on: 11 Dec 2024 19:02:26</div>
				</div>
				<div class="doc-toc">
					<ul>
						<li class="open"><a href="#readme_vxai">README</a></li>
						<li class="open"><a href="#Constants">Constants</a></li>
						<li class="open"><a href="#get_api_key_from_env">fn get_api_key_from_env</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#ChatCompletionInput.new">fn ChatCompletionInput.new</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#CompletionInput.new">fn CompletionInput.new</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#XAIClient.new">fn XAIClient.new</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#APIKeyInfo">struct APIKeyInfo</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#ChatCompletionChoice">struct ChatCompletionChoice</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#ChatCompletionInput">struct ChatCompletionInput</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#ChatCompletionMessage">struct ChatCompletionMessage</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#ChatCompletionResponse">struct ChatCompletionResponse</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#ChatCompletionUsage">struct ChatCompletionUsage</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#CompletionChoice">struct CompletionChoice</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#CompletionInput">struct CompletionInput</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#CompletionResponse">struct CompletionResponse</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#CompletionUsage">struct CompletionUsage</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#EmbeddingModel">struct EmbeddingModel</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#EmbeddingModelsResponse">struct EmbeddingModelsResponse</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#LanguageModel">struct LanguageModel</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#LanguageModelsResponse">struct LanguageModelsResponse</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#Model">struct Model</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#ModelsResponse">struct ModelsResponse</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#StreamChatCompletionChoice">struct StreamChatCompletionChoice</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#StreamChatCompletionChunk">struct StreamChatCompletionChunk</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#StreamChatCompletionDelta">struct StreamChatCompletionDelta</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#StreamChatCompletionPromptTokenDetails">struct
								StreamChatCompletionPromptTokenDetails</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#StreamChatCompletionUsage">struct StreamChatCompletionUsage</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#StreamCompletionChoice">struct StreamCompletionChoice</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#StreamCompletionChunk">struct StreamCompletionChunk</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#StreamCompletionDelta">struct StreamCompletionDelta</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#StreamCompletionPromptTokenDetails">struct
								StreamCompletionPromptTokenDetails</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#StreamCompletionUsage">struct StreamCompletionUsage</a>
							<ul>
							</ul>
						</li>
						<li class="open"><a href="#XAIClient">struct XAIClient</a>
							<ul>
								<li><a href="#XAIClient.get_api_key_info">fn get_api_key_info</a></li>
								<li><a href="#XAIClient.get_chat_completion">fn get_chat_completion</a></li>
								<li><a href="#XAIClient.get_completion">fn get_completion</a></li>
								<li><a href="#XAIClient.get_embedding_model">fn get_embedding_model</a></li>
								<li><a href="#XAIClient.get_embedding_models">fn get_embedding_models</a></li>
								<li><a href="#XAIClient.get_language_model">fn get_language_model</a></li>
								<li><a href="#XAIClient.get_language_models">fn get_language_models</a></li>
								<li><a href="#XAIClient.get_model">fn get_model</a></li>
								<li><a href="#XAIClient.get_models">fn get_models</a></li>
								<li><a href="#XAIClient.stream_chat_completion">fn stream_chat_completion</a></li>
								<li><a href="#XAIClient.stream_completion">fn stream_completion</a></li>
							</ul>
						</li>
						<li class="open"><a href="#XAIClientParams">struct XAIClientParams</a>
							<ul>
							</ul>
						</li>
					</ul>
				</div>
			</div>
		</div>
	</div>
	<script src="doc.js"></script>
	<script async src="search_index.js"></script>
</body>

</html>